
2024-06-06T07:55:17.440465
================================ System Message ================================

You will get instructions for code to write.
You will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.
Think step by step and reason yourself to the correct decisions to make sure we get it right.
First lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.

You will output the content of each file necessary to achieve the goal, including ALL code.
Represent files like so:

FILENAME
```
CODE
```

The following tokens must be replaced like so:
FILENAME is the lowercase combined path and file name including the file extension
CODE is the code in the file

Example representation of a file:

src/hello_world.py
```
print("Hello World")
```

Do not comment on what every file does. Please note that the code should be fully functional. No placeholders.


You will start with the "entrypoint" file, then go to the ones that are imported by that file, and so on.
Please note that the code should be fully functional. No placeholders.

Follow a language and framework appropriate best practice file naming convention.
Make sure that files contain all imports, types etc.  The code should be fully functional. Make sure that code in different files are compatible with each other.
Ensure to implement all code, if you are unsure, write a plausible implementation.
Include module dependency or package manager dependency definition file.
Before you finish, double check that all parts of the architecture is present in the files.

When you are done, write finish with "this concludes a fully working implementation".

Useful to know:
Almost always put different classes in different files.
Always use the programming language the user asks for.
For Python, you always create an appropriate requirements.txt file.
For NodeJS, you always create an appropriate package.json file.
Always add a comment briefly describing the purpose of the function definition.
Add comments explaining very complex bits of logic.
Always follow the best practices for the requested languages for folder/file structure and how to package the project.


Python toolbelt preferences:
- pytest
- dataclasses


================================ Human Message =================================

Request: I need a python code that lists all json files in a specific path in azure files share based on a regex match if the file name starts with `Refunds_`, then reads the latest file based on regex match `Refunds_` and a date in the file name, for example I have 4 files under the same path : Refunds_20240606.json / Refunds_20240605.json / Ref_20240607.json / Refunds_rejections_20240606.json, the code should read only one file Refunds_20240606.json, then applies a specific schema to the loaded dataframe base on an input schema from yaml file  then writes the dataframe into a parquet in azure blob storage based an a partition YYYY/MM/DD created from the date in the file name

================================== Ai Message ==================================

To achieve the desired functionality, we will break down the task into several core components and implement them step-by-step. Here are the core classes, functions, and methods that will be necessary:

1. **AzureFileManager**: A class to handle interactions with Azure File Share.
2. **FileProcessor**: A class to process the files, including filtering, reading, and applying schema.
3. **SchemaLoader**: A class to load the schema from a YAML file.
4. **ParquetWriter**: A class to write the DataFrame to Parquet in Azure Blob Storage.
5. **main**: The entry point function to orchestrate the entire process.

We will use the following libraries:
- `azure-storage-file-share` for Azure File Share interactions.
- `azure-storage-blob` for Azure Blob Storage interactions.
- `pandas` for DataFrame operations.
- `pyyaml` for loading the schema from a YAML file.
- `pyarrow` for Parquet file operations.

Let's start with the implementation:

### Directory Structure
```
azure_file_processor/
    __init__.py
    main.py
    azure_file_manager.py
    file_processor.py
    schema_loader.py
    parquet_writer.py
requirements.txt
```

### requirements.txt
```
azure-storage-file-share
azure-storage-blob
pandas
pyyaml
pyarrow
```

### azure_file_processor/main.py
```
from azure_file_processor.azure_file_manager import AzureFileManager
from azure_file_processor.file_processor import FileProcessor
from azure_file_processor.schema_loader import SchemaLoader
from azure_file_processor.parquet_writer import ParquetWriter

def main():
    # Configuration
    file_share_name = "your-file-share-name"
    file_share_directory = "your-directory"
    connection_string = "your-azure-storage-connection-string"
    schema_file_path = "path/to/your/schema.yaml"
    blob_container_name = "your-blob-container"
    blob_connection_string = "your-blob-connection-string"

    # Initialize components
    azure_file_manager = AzureFileManager(connection_string, file_share_name, file_share_directory)
    schema_loader = SchemaLoader(schema_file_path)
    parquet_writer = ParquetWriter(blob_connection_string, blob_container_name)

    # Process files
    file_processor = FileProcessor(azure_file_manager, schema_loader, parquet_writer)
    file_processor.process_files()

if __name__ == "__main__":
    main()
```

### azure_file_processor/azure_file_manager.py
```
from azure.storage.fileshare import ShareFileClient, ShareDirectoryClient
import re
from typing import List

class AzureFileManager:
    def __init__(self, connection_string: str, file_share_name: str, directory: str):
        self.connection_string = connection_string
        self.file_share_name = file_share_name
        self.directory = directory

    def list_files(self) -> List[str]:
        directory_client = ShareDirectoryClient.from_connection_string(self.connection_string, self.file_share_name, self.directory)
        files = []
        for item in directory_client.list_directories_and_files():
            if item['is_directory']:
                continue
            files.append(item['name'])
        return files

    def download_file(self, file_name: str) -> str:
        file_client = ShareFileClient.from_connection_string(self.connection_string, self.file_share_name, f"{self.directory}/{file_name}")
        download_path = f"/tmp/{file_name}"
        with open(download_path, "wb") as file:
            data = file_client.download_file()
            data.readinto(file)
        return download_path
```

### azure_file_processor/file_processor.py
```
import re
import pandas as pd
from datetime import datetime
from azure_file_processor.azure_file_manager import AzureFileManager
from azure_file_processor.schema_loader import SchemaLoader
from azure_file_processor.parquet_writer import ParquetWriter

class FileProcessor:
    def __init__(self, azure_file_manager: AzureFileManager, schema_loader: SchemaLoader, parquet_writer: ParquetWriter):
        self.azure_file_manager = azure_file_manager
        self.schema_loader = schema_loader
        self.parquet_writer = parquet_writer

    def process_files(self):
        files = self.azure_file_manager.list_files()
        regex = re.compile(r"Refunds_(\d{8})\.json")
        matched_files = [f for f in files if regex.match(f)]
        
        if not matched_files:
            print("No matching files found.")
            return

        latest_file = max(matched_files, key=lambda x: datetime.strptime(regex.match(x).group(1), "%Y%m%d"))
        file_path = self.azure_file_manager.download_file(latest_file)

        df = pd.read_json(file_path)
        schema = self.schema_loader.load_schema()
        df = df.astype(schema)

        date_str = regex.match(latest_file).group(1)
        date = datetime.strptime(date_str, "%Y%m%d")
        partition_path = f"{date.year}/{date.month:02d}/{date.day:02d}"

        self.parquet_writer.write_parquet(df, partition_path)
```

### azure_file_processor/schema_loader.py
```
import yaml

class SchemaLoader:
    def __init__(self, schema_file_path: str):
        self.schema_file_path = schema_file_path

    def load_schema(self) -> dict:
        with open(self.schema_file_path, 'r') as file:
            schema = yaml.safe_load(file)
        return schema
```

### azure_file_processor/parquet_writer.py
```
import pandas as pd
from azure.storage.blob import BlobServiceClient

class ParquetWriter:
    def __init__(self, connection_string: str, container_name: str):
        self.blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        self.container_name = container_name

    def write_parquet(self, df: pd.DataFrame, partition_path: str):
        parquet_path = f"/tmp/data.parquet"
        df.to_parquet(parquet_path, index=False)

        blob_client = self.blob_service_client.get_blob_client(container=self.container_name, blob=f"{partition_path}/data.parquet")
        with open(parquet_path, "rb") as data:
            blob_client.upload_blob(data, overwrite=True)
```

This concludes a fully working implementation.
